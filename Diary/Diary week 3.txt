Diary week 3

2018 5 Mar

Risk of bias if using built in function? When relying on in-built function cross_val_score to split the datasets, there's a chance that the randomisation introduces windows with more "padding" in either the training or testing dataset, introducing an artificial bias. It may therefore be ideal to split the proteins into datasets at the source, when parsing the text file. An idea is to parse the sequences and states into dictionaries, and draw random numbers from the dictionaries to split them into datasets.

Still noticing that any increment of windowsize past size 9 leads to depreciation in the accuracy score of the SVM. Why?

To try running a cross_val_score for entire dataset, with windowsize 9, and check state of the art predictor as to why size 9 is optimum.

To save a model, use joblib to save the trained SVM model as a .pkl file, after fitting the parameters to the clf.

Optimising the SVM -> To consider using gridsearch (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to optimise SVM parameters. But prior to embarking on this, to optimise ideal window size (either by testing using cross_val_score, or comparing against previous references).

Ran the cross-validation (cross_val_score) for entire dataset (375 proteins) using a 3 fold cross-validation and windowsize 9, which resulted in an accuracy measure of 0.69, taking a total of 230 minutes system time.

2018 6 Mar

Found a few references to appropriate window size, with conflicting information. Some mention use of window size 15 and some window size 9.

Best kernel to use? Need to be optimised.


2018 7 Mar

Dataset supposed to have relative file names (../)? Allow for input of testfile in terminal.

Output in terminal from predictor program supposed to print:
>ID
Seq
Topo

Trained model output_full.pkl is extremely large, 113MB, and cannot be pushed to github. A solution is to train the model on a partial part of the dataset instead. As such a short script "State_counter.py" was written to count the ratio of exposed/buried in parts of the dataset, to check if there's a significant bias or difference between different parts of the dataset. Testpart1 returned a ratio of 0.827, while testpart2 returned 0.849, and testpart3 0.81. It can be therefore concluded that the ratio of states between various parts of the dataset is fairly consistent, and training using a portion of the dataset may not lead to a less accurate result.

Trying to optimise predictor to parse and make predictions for multiple entries in a single text file (is that necessary?), encountering difficulties with extracting states based on sequence length, as topologies are returned as a single long string.

2018 8 Mar

To parse FASTA files, consider using Biopython. Biopython is also useful for connecting to NCBI servers for BLAST functions.

Completed the predictor using a model fitted for testsize50.txt, which parses a text file with multiple proteins and prints the output in the format specified yesterday.