Diary week 3

2018 5 Mar

Risk of bias if using built in function? When relying on in-built function cross_val_score to split the datasets, there's a chance that the randomisation introduces windows with more "padding" in either the training or testing dataset, introducing an artificial bias. It may therefore be ideal to split the proteins into datasets at the source, when parsing the text file. An idea is to parse the sequences and states into dictionaries, and draw random numbers from the dictionaries to split them into datasets.

Still noticing that any increment of windowsize past size 9 leads to depreciation in the accuracy score of the SVM. Why?

To try running a cross_val_score for entire dataset, with windowsize 9, and check state of the art predictor as to why size 9 is optimum.

To save a model, use joblib to save the trained SVM model as a .pkl file, after fitting the parameters to the clf.

Optimising the SVM -> To consider using gridsearch (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to optimise SVM parameters. But prior to embarking on this, to optimise ideal window size (either by testing using cross_val_score, or comparing against previous references).

Ran the cross-validation (cross_val_score) for entire dataset (375 proteins) using a 3 fold cross-validation and windowsize 9, which resulted in an accuracy measure of 0.69, taking a total of 230 minutes system time.

2018 6 Mar

Found a few references to appropriate window size, with conflicting information. Some mention use of window size 15 and some window size 9.

Best kernel to use? Need to be optimised.


2018 7 Mar

Dataset supposed to have relative file names (../)? Allow for input of testfile in terminal.

Output in terminal from predictor program supposed to print:
>ID

Seq

Predicted Topo

Trained model output_full.pkl is extremely large, 113MB, and cannot be pushed to github. A solution is to train the model on a partial part of the dataset instead. As such a short script "State_counter.py" was written to count the ratio of exposed/buried in parts of the dataset, to check if there's a significant bias or difference between different parts of the dataset. Testpart1 returned a ratio of 0.827, while testpart2 returned 0.849, and testpart3 0.81. It can be therefore concluded that the ratio of states between various parts of the dataset is fairly consistent, and training using a portion of the dataset may not lead to a less accurate result.

Trying to optimise predictor to parse and make predictions for multiple entries in a single text file (is that necessary?), encountering difficulties with extracting states based on sequence length, as topologies are returned as a single long string.

2018 8 Mar

To parse FASTA files, consider using Biopython. Biopython is also useful for connecting to NCBI servers for BLAST functions such as BLAST. Unfortunately biopython is incompatible with PSI-BLAST. As such BLAST+ was installed locally, and swissprot used as a database for conducting searches for PSI-BLAST.

In order to include PSSM data in the model, PSI-BLAST should be performed on each protein sequence in the dataset to return individual PSSM files. As such a python script should be written to parse the dataset file into individual FASTA files, followed by a bash script to automate the blasting of each FASTA file and saving of the PSSM matrix files.

Completed the predictor using a model fitted for testsize50.txt, which parses a text file with multiple proteins and prints the output in the format specified yesterday. To update readme with instructions to use predictor, as well as update predictor and model training scripts to use relative directories instead of hard-coded directories.

To perform more metric scoring (https://machinelearningmastery.com/machine-learning-in-python-step-by-step/) of the cross-valuation of the model using the size50 dataset. By specifying functions in the cross_val_score function, CVS returns scores of the test. Metrics to collect for classification include accuracy, average-precision, and a confusion matrix (good for presenting the accuracy of a classifier for 2 or more classes). Running an average_precision test for the testsize50 file gave a score of 0.78. Generating a confusion_matrix in SVMPredict.py returned the following matrix:

[[3988 2042]
 [2243 5353]]


Did more reading about optimisation of parameters used in fitting the model; C refers to the penalty C, kernel specifies the kernel to be used in separating the data (linear by default), while gamma specifies the sensitivity of the kernel to the differences in feature values. All these parameters are data-specific, and can be optimised using functions such as grid_search.

2018 9 Mar

Checked with David about the theory of using a subset for the model and training but a larger dataset should always be more accurate. Will consider proceeding to upload the full model on DropBox instead of Git, and perform GridSearch using the full model to optimise parameters as well.

Ran a train test split function in new script Datasplit.py that splits testfilesize50 after encoding, into x_train, x_test, y_train, and y_test. So a cross validation can be performed using this split dataset, for metric tests like Matthews Correlation Coefficient (MCC). Using testfilesize50 and a windowsize of 9 returned an MCC of 0.3652169895631994. What is the significance of this value?

Created a new script called randomforest.py that performs a cross_val_score on the RandomForestClassifier as opposed to SVM.SVC. Using a kfold of 3 to split the dataset, on testfilesize50 and windowsize 9 returns a mean accuracy score of 0.689, and a mean average precision score of 0.77.

Ran another round of cross_val_score with the same old parameters(not yet optimised)(windowsize9, testfilesize50, gamma=0.001, linear kernel, c=1), which returned a mean f1_score of 0.714.

2018 11 Mar

Found the optimum parameters for the model using GridSearchCV, with windowsize 21, C=5, gamma=0.01, and kernel=rbf.

To compare the f1_score between the old parameters and new parameters. The mean F1_score for the new parameters is 0.737. The mean f1_score for the old parameters was 0.714. Confusion matrix for the new parameters is:
[[3968 2062]
 [1968 5628]].

The previous number of false results was a total of 4285, vs 4030 with the new parameters.
