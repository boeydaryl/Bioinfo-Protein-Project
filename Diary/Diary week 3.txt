Diary week 3

2018 5 Mar

Risk of bias if using built in function? When relying on in-built function cross_val_score to split the datasets, there's a chance that the randomisation introduces windows with more "padding" in either the training or testing dataset, introducing an artificial bias. It may therefore be ideal to split the proteins into datasets at the source, when parsing the text file. An idea is to parse the sequences and states into dictionaries, and draw random numbers from the dictionaries to split them into datasets.

Still noticing that any increment of windowsize past size 9 leads to depreciation in the accuracy score of the SVM. Why?

To try running a cross_val_score for entire dataset, with windowsize 9, and check state of the art predictor as to why size 9 is optimum.

To save a model, use joblib to save the trained SVM model as a .pkl file, after fitting the parameters to the clf.

Optimising the SVM -> To consider using gridsearch (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to optimise SVM parameters. But prior to embarking on this, to optimise ideal window size (either by testing using cross_val_score, or comparing against previous references).

Ran the cross-validation (cross_val_score) for entire dataset (375 proteins) using a 3 fold cross-validation and windowsize 9, which resulted in an accuracy measure of 0.69, taking a total of 230 minutes system time.

2018 6 Mar

Found a few references to appropriate window size, with conflicting information. Some mention use of window size 15 and some window size 9.

Best kernel to use? Need to be optimised.


2018 7 Mar

Dataset supposed to have relative file names (../)? Allow for input of testfile in terminal.

Output in terminal from predictor program supposed to print:
>ID

Seq

Predicted Topo

Trained model output_full.pkl is extremely large, 113MB, and cannot be pushed to github. A solution is to train the model on a partial part of the dataset instead. As such a short script "State_counter.py" was written to count the ratio of exposed/buried in parts of the dataset, to check if there's a significant bias or difference between different parts of the dataset. Testpart1 returned a ratio of 0.827, while testpart2 returned 0.849, and testpart3 0.81. It can be therefore concluded that the ratio of states between various parts of the dataset is fairly consistent, and training using a portion of the dataset may not lead to a less accurate result.

Trying to optimise predictor to parse and make predictions for multiple entries in a single text file (is that necessary?), encountering difficulties with extracting states based on sequence length, as topologies are returned as a single long string.

2018 8 Mar

To parse FASTA files, consider using Biopython. Biopython is also useful for connecting to NCBI servers for BLAST functions such as BLAST. Unfortunately biopython is incompatible with PSI-BLAST. As such BLAST+ was installed locally, and swissprot used as a database for conducting searches for PSI-BLAST.

In order to include PSSM data in the model, PSI-BLAST should be performed on each protein sequence in the dataset to return individual PSSM files. As such a python script should be written to parse the dataset file into individual FASTA files, followed by a bash script to automate the blasting of each FASTA file and saving of the PSSM matrix files.

Completed the predictor using a model fitted for testsize50.txt, which parses a text file with multiple proteins and prints the output in the format specified yesterday. To update readme with instructions to use predictor, as well as update predictor and model training scripts to use relative directories instead of hard-coded directories.

To perform more metric scoring of the cross-valuation of the model using the size50 dataset. By specifying functions in the cross_val_score function, CVS returns scores of the test. Metrics to collect for classification include accuracy, average-precision, and a confusion matrix (good for presenting the accuracy of a classifier for 2 or more classes).

Did more reading about optimisation of parameters used in fitting the model; C refers to the penalty C, kernel specifies the kernel to be used in separating the data (linear by default), while gamma specifies the sensitivity of the kernel to the differences in feature values. All these parameters are data-specific, and can be optimised using functions such as grid_search.